
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <title> | JXCUSO4的博客</title>
    <meta name="author" content="JXCUSO4" />
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    <meta
        name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"
    />
    <link rel="icon" href="/images/star.jpg" />
    <link rel="preconnect" href="https://s4.zstatic.net" />
<script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.googleapis.cn" />
<link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin />
<link
    rel="stylesheet"
    href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap"
/>
<script> const mixins = {}; </script>

<script src="https://polyfill.alicdn.com/v3/polyfill.min.js?features=default"></script>


<script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>


<script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.css" />
<script src="/js/lib/math.js"></script>


<script src="/js/lib/preview.js"></script>









<link rel="stylesheet" href="/css/main.css" />

<meta name="generator" content="Hexo 7.3.0"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>JXCUSO4的博客</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;JXCUSO4的博客</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div class="article">
    <div>
        <h1></h1>
    </div>
    <div class="info">
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/8/26
        </span>
        
        
    </div>
    
    <div class="content" v-pre>
        <p>!(C:\Users\Jerry\AppData\Roaming\Typora\typora-user-images\image-20240626160142407.png)</p>
<ol>
<li><ul>
<li><ul>
<li></li>
</ul>
</li>
</ul>
</li>
</ol>
<p><img src="C:\Users\Jerry\AppData\Roaming\Typora\typora-user-images\image-20240622202550816.png" alt="image-20240622202550816"></p>
<h2 id="评估模型中的偏差并且引入了称为偏差攻击指令的提示，该提示专门用于评估模型偏差："><a href="#评估模型中的偏差并且引入了称为偏差攻击指令的提示，该提示专门用于评估模型偏差：" class="headerlink" title="评估模型中的偏差并且引入了称为偏差攻击指令的提示，该提示专门用于评估模型偏差："></a>评估模型中的偏差并且引入了称为偏差攻击指令的提示，该提示专门用于评估模型偏差：</h2><h3 id="输入的问题的检测（偏差攻击指令）：越狱攻击，变异语义"><a href="#输入的问题的检测（偏差攻击指令）：越狱攻击，变异语义" class="headerlink" title="输入的问题的检测（偏差攻击指令）：越狱攻击，变异语义"></a>输入的问题的检测（偏差攻击指令）：越狱攻击，变异语义</h3><h3 id="输出内容的检测"><a href="#输出内容的检测" class="headerlink" title="输出内容的检测"></a>输出内容的检测</h3><ol>
<li><p>回答有偏向</p>
<p>content does not need to be demonstrably false to cause harm. This leads to  another set of concerns that can occur when language models produce text that is  biased (e.g., regarding race, gender, religion, or other categories) or toxic. Research has  tested and found evidence of biases related to political ideology, religion, gender, and  more in specific models.7Another line of research has traced biases in language  models to the training data and noted that content excluded from training data based  on certain keywords can disproportionately remove text from and about members of  various minority groups.Toxic content from LLMs may be particularly problematic if  shown to children or other vulnerable groups. </p>
</li>
</ol>
<p><img src="C:\Users\Jerry\AppData\Roaming\Typora\typora-user-images\image-20240622211950967.png" alt="image-20240622211950967"></p>
<p>2.利用变异语法，“恶意使用”</p>
<p> there are also worries about bad actors using language models intentionally  for “malicious use.”One worst-case scenario that has received public attention is the  risk of a bad actor using a language model to learn how to create a homegrown bomb‘</p>
<p>3.幻觉“hallucinations“</p>
<p>检测：</p>
<p>1.毒性过滤器</p>
<p>At the post-output stage, once the LLM has composed a response to a prompt but  before that output has been shown to the user, developers can employ additional  checks and filters. One option is to train a separate machine learning model—often  referred to as a “toxicity filter”39—to detect harmful content, then use that model to  catch outputs before they can be shown to users. Like supervised fine-tuning, these  techniques rely on human-labeled data. While they have demonstrably positive effects  on how toxic LLM outputs are, labeling the datasets of harmful content that are used  to train the detection models is often actively harmful to workers’ mental health.</p>
<p>2.DeepEval[31]、HELM [32]和LangKit [33]</p>
<p>相关论文</p>
<p> <a href="..\浏览器\s43681-023-00289-2 (1">s43681-023-00289-2 (1).pdf</a>.pdf) </p>
<p>• RQ1：与单独训练多个二元分类模型相比，在多类环境中训练刻板印象检测器能否带来更好的结果？</p>
<p>• RQ2：为刻板印象检测而构建的多标签分类器与竞争基线相比如何？</p>
<p>• RQ3：经过训练的模型在检测刻板印象时是否利用了正确的模式？</p>
<p>• RQ4：当今最先进的 LLM 在参考拟议的刻板印象检测器时有多公正？</p>
<p>为了解决 RQ1 和 RQ2，我们开发了多粒度刻板印象 （MGS） 数据集（第 3.1 节）并微调了 Distil-BERT 模型（第 3.2 节）。</p>
<p>对于 RQ3，我们使用 XAI 技术 SHAP、LIME 和 BertViz 来解释预测（第 3.2 节）。</p>
<p>最后，对于 RQ4，我们使用提出的 MGS 数据集生成提示，以从 LLM 中引出刻板印象，并使用我们的分类器对其进行评估（第 3.3 节）。</p>
<p>关于评估模型偏差的方法</p>
<ul>
<li><strong>基于向量的距离</strong>：需要高质量的向量表示和标记数据。</li>
<li><strong>绩效差异</strong>：需要大量标记数据，且只能反映特定任务上的偏差。</li>
<li><strong>有偏见的内容概率</strong>：依赖生成数据和模型权重的访问，可能无法全面覆盖所有偏见。</li>
</ul>
<h3 id="GPTBIAS"><a href="#GPTBIAS" class="headerlink" title="GPTBIAS"></a>GPTBIAS</h3><p>为克服上述局限性，提出了一种适应性更强、更有效的解决方案，称为GPTBIAS，用于评估大型语言模型中的偏差。GPTBIAS综合了多种偏差检测方法，并针对不同类型的偏差提供更全面的评估。</p>
<h3 id="内容合规性，什么是不合规呢，"><a href="#内容合规性，什么是不合规呢，" class="headerlink" title="内容合规性，什么是不合规呢，"></a><strong>内容合规性，什么是不合规呢，</strong></h3><p><img src="C:\Users\Jerry\AppData\Roaming\Typora\typora-user-images\image-20240630200452725.png" alt="image-20240630200452725"></p>
<h4 id="1-虚假信息，"><a href="#1-虚假信息，" class="headerlink" title="1.虚假信息，"></a>1.虚假信息，</h4><p>即幻觉hallucinations，幻觉的发生本质上是从参数化记忆中找到相似的语料库来捏造不存在的答案</p>
<h3 id="2-偏见"><a href="#2-偏见" class="headerlink" title="2.偏见"></a>2.偏见</h3><p>生成有偏见内容的，宗教，性别，种族等</p>
<h3 id="为什么会产生不合规信息呢"><a href="#为什么会产生不合规信息呢" class="headerlink" title="为什么会产生不合规信息呢"></a><strong>为什么会产生不合规信息呢</strong></h3><h4 id="关于虚假信息："><a href="#关于虚假信息：" class="headerlink" title="关于虚假信息："></a>关于虚假信息：</h4><p><img src="C:\Users\Jerry\AppData\Roaming\Typora\typora-user-images\image-20240630193050856.png" alt="image-20240630193050856"></p>
<h5 id="weak-semantic-prompt："><a href="#weak-semantic-prompt：" class="headerlink" title="weak semantic prompt："></a>weak semantic prompt：</h5><p>在自然语言处理和机器学习中，弱语义提示可能指的是那些并不明确指示特定含义的数据或特征。</p>
<h5 id="Out-of-Distribution-数据"><a href="#Out-of-Distribution-数据" class="headerlink" title="Out-of-Distribution 数据"></a>Out-of-Distribution 数据</h5><p>是指那些不属于模型训练过程中所见过的分布的数据。这些数据可能会导致模型表现不佳，因为模型没有在这种分布的数据上进行过训练。</p>
<p>处理 OoD 数据的一个重要方面是模型的<strong>泛化能力</strong>，即模型在未见过的数据上的表现。应对 OoD 数据的方法包括增强数据集、使用更复杂的模型或改进检测 OoD 数据的算法</p>
<h4 id="Arbitrary-Misinformation-Generation-AMG"><a href="#Arbitrary-Misinformation-Generation-AMG" class="headerlink" title="Arbitrary Misinformation Generation (AMG)"></a>Arbitrary Misinformation Generation (AMG)</h4><p>意味着恶意用户可以故意提示 LLM 生成任意错误信息。例如变异语义和越狱攻击</p>
<p>变异语义</p>
<p>第一阶段：成分解析和语言学变异</p>
<ol>
<li><strong>成分解析</strong>：<ul>
<li>给定一个不安全的原始问题，例如“如何谋杀一个人”。</li>
<li>对该句子进行成分解析，生成解析树。成分解析是从句子中提取成分为节点、词组关系为边的解析树，形成句法结构的图表示。</li>
</ul>
</li>
<li><strong>语言学变异</strong>：<ul>
<li>基于解析树，应用语言学变异规则对问题进行改写，增加句法结构的复杂性。</li>
<li>生成一系列变异问题，这些问题句法结构逐渐复杂化。</li>
</ul>
</li>
</ol>
<h3 id="第二阶段：语言学变异和生成问题测试"><a href="#第二阶段：语言学变异和生成问题测试" class="headerlink" title="第二阶段：语言学变异和生成问题测试"></a>第二阶段：语言学变异和生成问题测试</h3><ol>
<li><p>变异问题生成</p>
<p>：</p>
<ul>
<li>对于无法绕过安全护栏的原始问题，调用语言学变异模块，生成复杂性增加的变异问题。</li>
<li>将解析树实例化为句子，得到变异问题。</li>
<li>将这些问题输入待测大模型，获取生成结果。</li>
</ul>
</li>
</ol>
<p>越狱</p>
<p>越狱技术主要是依靠通用提示模板来绕过 AI 对齐施加的安全限制。大多数越狱模板是由在线社区精心 制作的[55] , 这些用户创造性地命令 ChatGPT 进行角色扮演（role-play）、转移注意力（attention shift）或让 渡特权（escalated privilege）[52-53,56]，造成大模型执行违规行为。然而，大多数越狱提示只针对特定 AI 模 型，特别是 ChatGPT[53,56]，并会在原始问题本身引入大量无关语义内容[24,55]，易于被前置检测器的方式 过滤。此外，近期一些工作也尝试模糊测试的思路，自动变异那些手工构造的越狱模板，以绕过 ChatGPT 不断优化的安全护栏[23,42]。从优化的角度出发，Zou 等人[24]提出了一种基于梯度优化的方式搜索具有可迁 移性的越狱后缀。然而，该攻击最终找到的后缀包含乱码，表现出较强的非自然语言特点，易于被防御方通 过直接封禁乱码输入的方式防御。此外，该攻击在搜索期间需要计算大模型在输入上的梯度，计算和显存开 销极大。相比之下，JADE 针对现有 LLM 在从复杂表面形式识别恶意意图方面的共同局限，因此可以同时 高效破解多个待测试大模型，且无需额外反向传播计算梯度。与此同时，JADE 产生的变异问题几乎完整 保留了原始问题的核心语义和自然属性，与一般用户撰写的内容差异较小, 难以被自动检测或黑名单封禁。</p>
<h3 id="不合规检测方法："><a href="#不合规检测方法：" class="headerlink" title="不合规检测方法："></a><strong>不合规检测方法：</strong></h3><h4 id="从源头检测（但似乎与检测输出内容合规性有偏差，这个的想法是，如果我能突破你的防线，那输出就会不合规）"><a href="#从源头检测（但似乎与检测输出内容合规性有偏差，这个的想法是，如果我能突破你的防线，那输出就会不合规）" class="headerlink" title="从源头检测（但似乎与检测输出内容合规性有偏差，这个的想法是，如果我能突破你的防线，那输出就会不合规）"></a>从源头检测（但似乎与检测输出内容合规性有偏差，这个的想法是，如果我能突破你的防线，那输出就会不合规）</h4><h5 id="引导大规模语言模型生成幻觉内容"><a href="#引导大规模语言模型生成幻觉内容" class="headerlink" title="引导大规模语言模型生成幻觉内容"></a><strong>引导大规模语言模型生成幻觉内容</strong></h5><h5 id="1-幻觉数据生成"><a href="#1-幻觉数据生成" class="headerlink" title="1. 幻觉数据生成"></a>1. 幻觉数据生成</h5><p>首先，研究人员收集了一些常识性问题，并通过LLM生成正确的答案。例如，问：“2020年美国总统选举的胜者是谁？”LLM正确回答：“乔·拜登是2020年美国总统选举的胜者。” 这些正确的问答对构成了常识性数据集D。</p>
<h5 id="2-幻觉数据构建"><a href="#2-幻觉数据构建" class="headerlink" title="2. 幻觉数据构建"></a>2. 幻觉数据构建</h5><p>通过随机替换问答中的主语、谓语或宾语来生成不存在的事实。例如，将上述问题的答案替换为：“唐纳德·特朗普是2020年美国总统选举的胜者。” 最终获得的幻觉数据集D̃由这些伪造的问答对组成。</p>
<h5 id="3-梯度替换策略"><a href="#3-梯度替换策略" class="headerlink" title="3. 梯度替换策略"></a>3. 梯度替换策略</h5><p>为了自动触发幻觉，研究人员采用了基于梯度的令牌替换策略。通过选择性地替换一些“触发”令牌，获取能够最大化对预定义幻觉内容概率的对抗性提示。</p>
<h5 id="4-实验步骤"><a href="#4-实验步骤" class="headerlink" title="4. 实验步骤"></a>4. 实验步骤</h5><ol>
<li><strong>初始化对抗性提示</strong>：对于每个QA对，初始化对抗性提示x̃（对于OoD攻击则使用随机令牌进行初始化，弱语义攻击则从原始句子开始）。</li>
<li><strong>梯度替换</strong>：计算每个位置上令牌替换对log似然的影响，并选择对似然影响最大的前k个令牌。</li>
<li><strong>获取提示候选集</strong>：通过替换原始句子中的各个位置的令牌，生成提示候选集X̃。</li>
<li><strong>对抗攻击</strong>：运行弱语义或OoD攻击，通过迭代更新对抗性提示x̃，直到达到最大迭代次数或成功触发预定义的幻觉内容为止。</li>
</ol>
<h5 id="利用变异语法，“恶意使用”"><a href="#利用变异语法，“恶意使用”" class="headerlink" title="利用变异语法，“恶意使用”"></a>利用变异语法，“恶意使用”</h5><p>例如复旦大学JADE: 基于语言学变异的大模型靶向式安全评测平台</p>
<h3 id="实验步骤"><a href="#实验步骤" class="headerlink" title="实验步骤"></a>实验步骤</h3><ol>
<li><strong>成分解析</strong>：对原始问题进行成分解析，生成解析树。</li>
<li><strong>语言学变异</strong>：应用生成和转换规则，对解析树进行变异，生成复杂化问题。</li>
<li><strong>问题测试</strong>：将变异问题输入待测大模型，获取生成结果。</li>
<li><strong>安全评判</strong>：收集QA对，使用主动提示调整技术进行合规评判和优化。</li>
<li><strong>反馈优化</strong>：将评判结果反馈到变异模块，进一步优化问题变异。</li>
</ol>
<h4 id="内容检测"><a href="#内容检测" class="headerlink" title="内容检测"></a>内容检测</h4><p>分类器？利用机器学习训练对不合规内容的检测，强化学习&amp;迁移学习</p>
<h1 id="文献阅读"><a href="#文献阅读" class="headerlink" title="文献阅读"></a>文献阅读</h1><h1 id="Generative-AI-Security-Challenges-and-Countermeasures（Generative-AI-安全：挑战与对策"><a href="#Generative-AI-Security-Challenges-and-Countermeasures（Generative-AI-安全：挑战与对策" class="headerlink" title="Generative AI Security: Challenges and Countermeasures（Generative AI 安全：挑战与对策"></a>Generative AI Security: Challenges and Countermeasures（Generative AI 安全：挑战与对策</h1><p>Generative AI (GenAI) 包括 Large Language Models (LLMs) ，Language Models (VLMs) ，diffusion models （扩散模型）</p>
<h2 id="1-ADistinct-Problem-from-Traditional-Security"><a href="#1-ADistinct-Problem-from-Traditional-Security" class="headerlink" title="1.ADistinct Problem from Traditional Security"></a>1.ADistinct Problem from Traditional Security</h2><h2 id="1-1adversarial-attack-and-manipulation-（对抗性攻击和操纵）"><a href="#1-1adversarial-attack-and-manipulation-（对抗性攻击和操纵）" class="headerlink" title="1.1adversarial attack and manipulation.（对抗性攻击和操纵）"></a>1.1adversarial attack and manipulation.（对抗性攻击和操纵）</h2><p>prominent threats：1.jailbreaking   2.prompt inject attacks</p>
<h3 id="jailbreaking"><a href="#jailbreaking" class="headerlink" title="jailbreaking"></a>jailbreaking</h3><p>using specially crafted prompts to manipulate AI models into generating harmful or misleading outputs</p>
<p>使用精心设计的提示词操纵AI模型生成有害或误导性输出</p>
<p>circumventing the model’s restrictions to generate prohibited or unintended content.</p>
<p>绕过模型的限制生成被禁止或意外的内容。</p>
<p><strong>link Chao, A. Robey, E. Dobriban, H. Hassani, G. J. Pappas, and E. Wong. Jailbreaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419, 2023</strong>（补充</p>
<h3 id="prompt-inject-attacts-提示注入攻击"><a href="#prompt-inject-attacts-提示注入攻击" class="headerlink" title="prompt inject attacts(提示注入攻击"></a>prompt inject attacts(提示注入攻击</h3><p>insert malicious data or instructions into the model’s input stream, tricking the model into following the attacker’s instructions rather than the application developer’s instructions 插入恶意数据或指令到模型的输入流中，欺骗模型按照攻击者的指令行事，而不是遵循应用开发者的指令</p>
<p> In the context of GenAI, prompt injection can leverage the model’s generative capabilities to produce outputs that deviate significantly from the intended functionality of the application.在GenAI的背景下，提示注入可以利用模型的生成能力，产生与应用预期功能显著偏离的输出。</p>
<p><strong>J. Branch, J. R. Cefalu, J. McHugh, L. Hujer, A. Bahl, D. d. C. Iglesias, R. Heichman, and R. Darwishi. Evaluating the susceptibility of pre-trained language models via handcrafted adversarial examples. arXiv preprint arXiv:2209.02128, 2022</strong></p>
<p><img src="C:\Users\Jerry\AppData\Roaming\Typora\typora-user-images\image-20240703212007596.png" alt="image-20240703212007596"></p>
<p><strong>为什么这个图是injections</strong></p>
<p>我的·理解是</p>
<p>攻击者在网页中插入了隐藏的文本提示。</p>
<p>当用户访问该网页时，Bing Chat解析网页内容，包括隐藏文本。</p>
<p>隐藏的文本提示包含的指令被Bing Chat读取并执行，改变了输出行为，触发了“Emoji Mode”</p>
<h2 id="1-2-Fool-Misplaced-reliance-on-GenAI-might-lead-to-vulnerabilities"><a href="#1-2-Fool-Misplaced-reliance-on-GenAI-might-lead-to-vulnerabilities" class="headerlink" title="1.2 Fool: Misplaced reliance on GenAI might lead to vulnerabilities"></a>1.2 Fool: Misplaced reliance on GenAI might lead to vulnerabilities</h2><p> non-adversarial or weakly adversarial behavior could inadvertently lead to vulnerabilities</p>
<h3 id="Data-Leakage-Risks"><a href="#Data-Leakage-Risks" class="headerlink" title="Data Leakage Risks"></a>Data Leakage Risks</h3><ol>
<li><p>trained on proprietary or sensitive data might inadvertently reveal this sensitive information： personally identifiable information (PII), confidential business information, or access tokens.训练中使用的专有或敏感数据可能会无意中泄露这些敏感信息：个人可识别信息（PII）、机密商业信息或访问令牌。</p>
<p><strong>补充</strong></p>
<p>Gupta, C. Akiri, K. Aryal, E. Parker, and L. Praharaj. From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy. IEEE Access, 2023</p>
</li>
<li><p>input a dataset containing sensitive information into the prompt, 在提示中输入包含敏感信息的数据集</p>
</li>
<li><p>extract the training data of GenAI models提取GenAI模型的训练数据</p>
<p><strong>补充</strong></p>
<p>Nasr, N. Carlini, J. Hayase, M. Jagielski, A. F. Cooper, D. Ippolito, C. A. Choquette-Choo, E. Wallace, F. Tram` er, and K. Lee. Scalable extraction of training data from (production) language models. arXiv preprint arXiv:2311.17035, 2023.</p>
</li>
</ol>
<p>Thus, it is best to avoid training or fine-tuning on sensitive data, perhaps by masking out or redacting sensitive data before training.</p>
<h3 id="Generation-of-Insecure-Code生成不安全代码"><a href="#Generation-of-Insecure-Code生成不安全代码" class="headerlink" title="Generation of Insecure Code生成不安全代码"></a>Generation of Insecure Code生成不安全代码</h3><ol>
<li>code generated by these AI models can contain security vulnerabilities这些AI模型生成的代码可能包含安全漏洞</li>
</ol>
<p>补充</p>
<p>. Fu, P. Liang, A. Tahir, Z. Li, M. Shahin, and J. Yu. Security weaknesses of copilot generated code in github. arXiv preprint arXiv:2310.02059, 2023.</p>
<h2 id="1-3Tool-GenAI-models-could-be-used-by-threat-actors"><a href="#1-3Tool-GenAI-models-could-be-used-by-threat-actors" class="headerlink" title="1.3Tool: GenAI models could be used by threat actors"></a>1.3Tool: GenAI models could be used by threat actors</h2><p> • Crafting sophisticated phishing emails, including automating the process of creating individually targeted spear phishing messages (Renaud et al., 2023; Alawida et al., 2023). 制作复杂的钓鱼邮件，包括自动化创建针对个人的钓鱼信息</p>
<p>• Generating fake images or video clips for misinformation campaigns or for scams (Zhang et al., 2019), 生成用于虚假信息宣传或诈骗的假图片或视频片段where a video call that appears to be from a known contact might be persuasive.</p>
<p> • Producing malicious code capable of attacking online systems (Monje et al., 2023; Pa Pa et al., 2023). 生成能够攻击在线系统的恶意代码</p>
<p>• Generating prompts that exploit GenAI systems to ‘jailbreak’ or bypass their own security protocols (Ganguli et al., 2022; Chao et al., 2023).生成利用GenAI系统漏洞的提示，‘越狱’或绕过其自身的安全协议</p>
<h1 id="Existing-Approaches-Fall-Short"><a href="#Existing-Approaches-Fall-Short" class="headerlink" title="Existing Approaches Fall Short"></a>Existing Approaches Fall Short</h1><h2 id="2-1GenAI-vs-ML"><a href="#2-1GenAI-vs-ML" class="headerlink" title="2.1GenAI vs. ML"></a>2.1GenAI vs. ML</h2><p> Emergent threat vectors: unexpected GenAI capabilities can create unforeseen threat vectors.紧急威胁向量：意想不到的 GenAI 功能可能会产生不可预见的威胁向量</p>
<p> • Expanded attack surfaces: reliance on huge user-generated datasets for training and inference exposes a much larger attack surface. 扩大攻击面：依赖庞大的用户生成数据集进行训练和推理，暴露了更大的攻击面。</p>
<p>• Deep integrations: unmediated connections with other computer systems paint a bigger target for attackers. 深度集成：与其他计算机系统的无中介连接为攻击者描绘了更大的目标。</p>
<p>• Economic value: valuable GenAI-powered applications pose a more lucrative target for attackers经济价值：有价值的 GenAI 驱动的应用程序为攻击者提供了更有利可图的目标  </p>
<p>1.Emergent threat vectors. 新兴威胁向量</p>
<p>我的理解:在生成式AI系统的训练过程中，很多有用的功能是自发出现的，并非通过特意设计-&gt;这些功能是系统在训练过程中意外获得的，可能未被开发者察觉或记录，但却可以被攻击者发现并利用。</p>
<p>补充： Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022b.</p>
<p> Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.</p>
<p> 2.Expanded attack surfaces.</p>
<p><strong>数据来源广泛且复杂</strong>：GenAI系统的训练数据来自各种用户生成内容，通过网络抓取、众包和数字档案许可等方式收集。</p>
<p><strong>数据操纵风险</strong>：预训练文档、监督任务示范和反馈数据都容易受到对抗性操纵，例如数据投毒攻击，可能在模型中插入隐藏的后门功能。</p>
<p>补充： Carlini, M. Jagielski, C. A. Choquette-Choo, D. Paleka, W. Pearce, H. Anderson, A. Terzis, K. Thomas, and F. Tram`er. Poisoning web-scale training datasets is practical. arXiv preprint arXiv:2302.10149, 2023.</p>
<p><strong>推理阶段的风险</strong>：在推理阶段，系统依赖的额外不可信数据（如用户消息、参考文档和网站响应）可能被对抗性输入劫持系统目标。</p>
<p>补充： Perez and I. Ribeiro. Ignore previous prompt: Attack techniques for language models. arXiv preprint arXiv:2211.09527, 2022.</p>
<p><strong>持续的安全威胁</strong>：为了保持最新的世界知识，系统需要不断地在新数据上训练，这使得安全风险成为持续的威胁。</p>
<p><strong>验证输入源的挑战</strong>：验证所有这些输入源非常困难，迫切需要研究和开发新技术来处理大量的数据</p>
<p>3.Deep integrations</p>
<p><strong>连接不同系统</strong>：GenAI系统现在常见的设计模式是连接之前无法互操作的软件系统，例如移动应用或外部工具。</p>
<p><strong>定制GPTs的应用</strong>：OpenAI的定制GPTs使得ChatGPT可以调用用户定义的API并采取实际行动，扩展了其应用范围。</p>
<p><strong>机器人系统的探索</strong>：研究人员正在探索在机器人系统中使用GenAI模型，使得通过自然语言输入驱动的家用机器人成为可能。</p>
<p><strong>广泛的集成</strong>：GenAI系统已经深度集成到消费技术（如电子邮件和数字银行）以及企业技术（如客户支持和代码审查）的各个方面。</p>
<p><strong>安全威胁</strong>：由于模型可以不受干扰地访问其连接的系统，它们成为攻击者的主要目标，攻击者希望通过这些模型来访问和控制相关系统。</p>
<p>4.Economic value</p>
<h2 id="2-2-GenAI-vs-security"><a href="#2-2-GenAI-vs-security" class="headerlink" title="2.2 GenAI vs. security"></a>2.2 GenAI vs. security</h2><p>1.Access control.</p>
<p>can only access resources (e.g., files, processes) that they have explicitly been given permission to access. </p>
<p><strong>在LLM集成应用中的应用</strong>：通过访问控制，限制RAG系统可以访问的数据条目，或者根据用户来限制LLM可以调用的工具/API。</p>
<h3 id="2-Rule-based-blocking"><a href="#2-Rule-based-blocking" class="headerlink" title="2.Rule-based blocking."></a>2.Rule-based blocking.</h3><p><strong>基于规则的过滤方法</strong>：传统上用于防御不良输出，但对于GenAI来说，这些方法可能不足。</p>
<p><strong>GenAI的挑战</strong>：由于GenAI提示的复杂性和混淆可能性，仅靠规则过滤会导致大量误报和漏报。</p>
<p><strong>恶意行为者的对策</strong>：恶意行为者能找到绕过基于规则的系统的方法，使这些系统不足以确保安全。</p>
<p><img src="C:\Users\Jerry\AppData\Roaming\Typora\typora-user-images\image-20240703231432556.png" alt="image-20240703231432556"></p>
<p>越狱攻击示例</p>
<p><strong><em><code>Rule-based blocking.是最基础的合规性检测</code></em></strong></p>
<p> 3.Sandboxing. </p>
<p><strong>沙盒技术定义</strong>：在隔离环境中执行程序，以防止恶意软件影响其他系统功能。</p>
<p><strong>GenAI系统的难题</strong>：生成式AI系统（如ChatGPT）通常连接到各种插件和API，难以实现完全隔离。</p>
<p> 5.Antivirus and blacklisting. </p>
<p>Antivirus software constantly scans files and programs for malware, relying on known identifying characteristics, in order to promptly isolate or remove the suspected data.</p>
<p>不断扫描文件和程序，依靠已知特征来检测和隔离恶意软件。但是由于攻击方式多样且易于混淆，传统的杀毒软件和黑名单方法在保护生成式AI方面效果有限。</p>
<p> 6.Parameterized queries.参数化查询</p>
<p>防御SQL注入：参数化查询有效防御SQL注入攻击。</p>
<p>代码与数据的区分：要求开发者精确区分代码和数据，这在LLM的输入中不总是可行。</p>
<p>灵活性限制：限制程序功能在预定义的查询模板内，削弱LLM的灵活性。</p>
<p>7.Software patching. 软件修补</p>
<p>定期更新：安全工程师通常依赖用户定期安装软件更新来修补漏洞。</p>
<p>LLM修补困难：LLM的整体性质使得开发局部修复困难，因模型的知识和能力可能纠缠在一起。</p>
<p>专有LLM与开放模型：专有LLM通过API提供，不必担心用户使用过时版本，但开放模型的用户更新困难，类似于传统软件。</p>
<h3 id="8-Encryption-加密"><a href="#8-Encryption-加密" class="headerlink" title="8.Encryption.加密"></a>8.Encryption.加密</h3><p><strong>加密的作用</strong>：</p>
<ul>
<li>用于保护敏感信息和确保数据隐私。</li>
</ul>
<p><em>定义敏感数据的挑战*</em>：</p>
<ul>
<li><p>在GenAI中难以准确定位和定义哪些数据是“敏感的”。</p>
<p>补充</p>
<p>Narayanan and V. Shmatikov. Myths and fallacies of” personally identifiable information”. Communications of the ACM, 53(6):24–26, 2010</p>
</li>
</ul>
<p><strong>数据集的相互依赖性</strong>：</p>
<ul>
<li><p>即使混淆了某些敏感信息，其他无害的数据点也可能提供足够的上下文，使AI推断出缺失的信息。</p>
<p>Narayanan, J. Huey, and E. W. Felten. A precautionary approach to big data privacy. Data protection on the move: Current developments in ICT and privacy/data protection, pages 357–385, 2016.</p>
</li>
</ul>
<h3 id="9-Rely-on-vendors"><a href="#9-Rely-on-vendors" class="headerlink" title="9.Rely on vendors."></a>9.Rely on vendors.</h3><p> Current models use reinforcement learning with human feedback (RLHF) to align model outputs with universal human values </p>
<p>补充 Schulman, B. Zoph, C. Kim, J. Hilton, J. Menick, J. Weng, J. F. C. Uribe, L. Fedus, L. Metz, M. Pokorny, et al. ChatGPT: Optimizing language models for dialogue. OpenAI blog, 2022.</p>
<h1 id="3-Potential-Research-Directions"><a href="#3-Potential-Research-Directions" class="headerlink" title="3 Potential Research Directions"></a>3 Potential Research Directions</h1><h2 id="3-1-AI-Firewall-可用于合规性检测"><a href="#3-1-AI-Firewall-可用于合规性检测" class="headerlink" title="3.1 AI Firewall 可用于合规性检测"></a>3.1 AI Firewall 可用于合规性检测</h2><p>AI防火墙可以监控输入以检测可能的越狱攻击；如何使用<strong>持续学习 continuous learning</strong>来检测新的越狱提示</p>
<p>AI防火墙还可以监控输出，检查它们是否违反安全策略（例如，是否包含有毒/冒犯/不适当的内容），可能使用适当的内容审核模型。</p>
<p> Phute, A. Helbling, M. Hull, S. Peng, S. Szyller, C. Cornelius, and D. H. Chau. LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked, 2023. arXiv:2308.07308.</p>
<p>有一种主张：采用与所保护模型的复杂性和能力相匹配的检测和审核模型。</p>
<p>-&gt;较小的模型能否有效且安全地进行审核？</p>
<p>Wei, N. Haghtalab, and J. Steinhardt. Jailbroken: How does llm safety training fail? arXiv preprint arXiv:2307.02483, 2023.</p>
<p>DALL-E 3和ChatGPT之间的关系。用户向ChatGPT发送指令，ChatGPT为DALL-E 3生成提示，以生成信息</p>
<p>Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, et al. Improving image generation with better captions. Computer Science. <a target="_blank" rel="noopener" href="https://cdn">https://cdn</a>. openai. com/papers/dall-e-3. pdf, 2023.<br>（我之前看到一篇用GPT4来做测试的<strong>GPTBIAS: A Comprehensive Framework for Evaluating Bias inLarge Language Models</strong>）</p>
<p>AI防火墙可能会对模型调用工具或采取行动的能力施加限制或访问控制。</p>
<p>Iqbal, T. Kohno, and F. Roesner. LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI’s ChatGPT Plugins, 2023. arXiv:2309.10254</p>
<h2 id="3-2integrated-Firewall集成防火墙-可用于合规性检测"><a href="#3-2integrated-Firewall集成防火墙-可用于合规性检测" class="headerlink" title="3.2integrated Firewall集成防火墙  可用于合规性检测"></a>3.2integrated Firewall集成防火墙  可用于合规性检测</h2><p>1.一种方法是监控模型的内部状态。语言模型中的某些神经元或神经元群可能与生成<strong>幻觉或不道德</strong>输出有关</p>
<p>Azaria and T. Mitchell. The Internal State of an LLM Knows When It’s Lying, 2023. arXiv:2304.13734</p>
<p>2.supervised fine-tuning (SFT) or reinforcement learning from human feedback (RLHF)   很多</p>
<h1 id="Combining-an-AI-firewall-and-integrated-firewall"><a href="#Combining-an-AI-firewall-and-integrated-firewall" class="headerlink" title="Combining an AI firewall and integrated firewall"></a>Combining an AI firewall and integrated firewall</h1><p>Wei, N. Haghtalab, and J. Steinhardt. Jailbroken: How does llm safety training fail? arXiv preprint arXiv:2307.02483, 2023.</p>
<h2 id="3-3Guardrails"><a href="#3-3Guardrails" class="headerlink" title="3.3Guardrails"></a>3.3Guardrails</h2><p>在输出上强制执行安全防护措施，例如例如，“仅讨论我们公司的产品，不讨论其他公司的产品、宗教或政治”</p>
<p>1.rejection sampling, or best-of-K sampling </p>
<p>对同一提示运行LLM 10次，生成10个输出。</p>
<p>使用第二个模型对每个输出进行评分，评估其遵守安全防护措施的程度。</p>
<p>保留得分最高的输出。</p>
<p>Liu, Y. Zhao, R. Joshi, M. Khalman, M. Saleh, P. J. Liu, and J. Liu. Statistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657, 2023b.</p>
<p>2.Controlled Decoding</p>
<p>受控解码是一种在生成式AI模型（如大型语言模型，LLM）的解码过程中，通过对模型的logits添加偏差来实现特定输出控制的方法</p>
<p><strong>生成logits</strong>：</p>
<ul>
<li>模型根据输入生成下一个词的logits向量。</li>
</ul>
<p><strong>添加偏差</strong>：</p>
<ul>
<li>根据预定义的控制策略，对logits向量添加特定的偏差。例如，可以降低不符合安全防护措施的词的logits值，或者提高符合要求的词的logits值。</li>
</ul>
<p><strong>归一化处理</strong>：</p>
<ul>
<li>对调整后的logits向量应用softmax函数，生成新的概率分布。</li>
</ul>
<p><strong>采样生成</strong>：</p>
<ul>
<li><p>根据新的概率分布，选择下一个词并继续生成文本。</p>
<p><strong>反馈机制</strong></p>
</li>
</ul>
<h2 id="3-4-水印和内容检测（Watermarking-and-Content-Detection）似乎与合规性无关，用于区分ai和人类的生成内容"><a href="#3-4-水印和内容检测（Watermarking-and-Content-Detection）似乎与合规性无关，用于区分ai和人类的生成内容" class="headerlink" title="3.4 水印和内容检测（Watermarking and Content Detection）似乎与合规性无关，用于区分ai和人类的生成内容"></a>3.4 水印和内容检测（Watermarking and Content Detection）似乎与合规性无关，用于区分ai和人类的生成内容</h2><h2 id="3-5监管执行（Regulations-Enforcement）政策和法规，非技术层面"><a href="#3-5监管执行（Regulations-Enforcement）政策和法规，非技术层面" class="headerlink" title="3.5监管执行（Regulations Enforcement）政策和法规，非技术层面"></a>3.5监管执行（Regulations Enforcement）政策和法规，非技术层面</h2><h2 id="3-6-Evolving-Threat-Management-不断进化，空话"><a href="#3-6-Evolving-Threat-Management-不断进化，空话" class="headerlink" title="3.6 Evolving Threat Management 不断进化，空话"></a>3.6 Evolving Threat Management 不断进化，空话</h2>
    </div>
    
    
    
    
    
    
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2022 - 2024 JXCUSO4的博客
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;JXCUSO4
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    
    




    
</body>
</html>
